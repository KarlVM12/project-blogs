<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Karl Muller</title>
    <link>/</link>
    <description>Recent content on Karl Muller</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 04 May 2025 17:54:46 -0400</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Language &amp; LLMs = Expression, not Intelligence</title>
      <link>/posts/llms-are-expression-not-intelligence/</link>
      <pubDate>Sun, 04 May 2025 17:54:46 -0400</pubDate>
      
      <guid>/posts/llms-are-expression-not-intelligence/</guid>
      <description>&lt;p&gt;LLMs have been lauded for having intelligence and reasoning capabilities simply from choosing the next probable word in a sequence &lt;a href=&#34;#ref1&#34;&gt;[1]&lt;/a&gt;.
Formally, the APA defines intelligence as &amp;ldquo;the ability to derive information, learn from experience, adapt to the environment, understand, and correctly utilize thought and reason.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Language is certainly a powerful tool for communicating, spreading ideas, and technological advancement. Without language, ideas could never have moved as fast as they currently do. With this, some would then deduce that language then inherently leads towards intelligence, but this is simply not true. I do not study quantum mechanics, but I can read, memorize, and recite a proof if given enough to read, enough time to memorize, and a medium in order to regurgitate the information (speaking/writing English in this case). But that in no means actually proves I am intelligent or understand what I am saying or writing. With enough different sources of information and time to put it all together, I could probably also string together different memorized pieces of information to reach another piece of information, without ever having to actually understand any of it. In Searle&amp;rsquo;s Chinese Room Argument, a person inside a room can manipulate Chinese symbols following rules without ever actually understanding Chinese &lt;a href=&#34;#ref2&#34;&gt;[2]&lt;/a&gt;. This shows how language &amp;amp; syntax is not equal to semantics and understanding.&lt;/p&gt;</description>
      <content>&lt;p&gt;LLMs have been lauded for having intelligence and reasoning capabilities simply from choosing the next probable word in a sequence &lt;a href=&#34;#ref1&#34;&gt;[1]&lt;/a&gt;.
Formally, the APA defines intelligence as &amp;ldquo;the ability to derive information, learn from experience, adapt to the environment, understand, and correctly utilize thought and reason.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Language is certainly a powerful tool for communicating, spreading ideas, and technological advancement. Without language, ideas could never have moved as fast as they currently do. With this, some would then deduce that language then inherently leads towards intelligence, but this is simply not true. I do not study quantum mechanics, but I can read, memorize, and recite a proof if given enough to read, enough time to memorize, and a medium in order to regurgitate the information (speaking/writing English in this case). But that in no means actually proves I am intelligent or understand what I am saying or writing. With enough different sources of information and time to put it all together, I could probably also string together different memorized pieces of information to reach another piece of information, without ever having to actually understand any of it. In Searle&amp;rsquo;s Chinese Room Argument, a person inside a room can manipulate Chinese symbols following rules without ever actually understanding Chinese &lt;a href=&#34;#ref2&#34;&gt;[2]&lt;/a&gt;. This shows how language &amp;amp; syntax is not equal to semantics and understanding.&lt;/p&gt;
&lt;p&gt;When we apply this to LLMs, we can see this is also true. With enough time to train and tweak their parameters, weights, and biases, they can generate that information in their own medium. Once again, however, this does not equate to intelligence with the same following as the previous conclusion. Language in all cases serves only as a medium to transmit information and ideas, as a means of expression. LLMs can express and perform what some call ‘reasoning’ (explored below) across as many ideas as they want, but that in no way proceeds to assume intelligence, and especially far from Artificial Intelligence.&lt;/p&gt;
&lt;p&gt;Since we can see language as a means of expression simply does not, and can not by itself, lead to understanding, there is also the topic of tackling what is currently called ‘reasoning’ in the AI hype zeitgeist. Reasoning as it currently works in LLMs string together the next most probable strings of text based on its training, as well as throwing a couple of tricks in. Adding these tricks does not mean it is actually reasoning by any means. This perceived &amp;lsquo;reasoning&amp;rsquo; only exists simply because people were extending the also misnomer of &lt;em&gt;Chain of Thought&lt;/em&gt;. There might be a chain of words being strung together, but no thought process is actually occurring. For example, a simple trick of inserting ‘Let&amp;rsquo;s think step by step’ in the sequence of probabilities has resulted in an increased retrieval of a correct answer in LLMs &lt;a href=&#34;#ref3&#34;&gt;[3]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This is of course why prompt engineering works so well. If you insert a token of doubt into an incorrect answer, it will lean the probability towards a more correct answer. Since each token influences the conditional probability of the next, adjusting earlier tokens can shift the model’s output distribution toward more probable completions, and less towards &amp;lsquo;hallucinations&amp;rsquo;, as the anthropomorphized term goes. Therefore, when you throw a ‘wait’ &lt;a href=&#34;#ref4&#34;&gt;[4]&lt;/a&gt; in the following sequence, it leans even further in towards what would be the most correct probability according to its training. Tricks like these are by no means reasoning. Stringing together more probabilities will of course lead to improved retrieval because you start to touch on more of the parameters and tokens associated with the prompt. With more context and tokens surrounding a prompt, after all the so called ‘reasoning’ process, that will inherently lead to a better answer. So there is no actual reasoning going on, just more compute and tokens leveraged with a trick being spit out to reach that final answer.&lt;/p&gt;
&lt;p&gt;To round this out, language does not equate to understanding, and is far from intelligence. Regurgitating and retrieving more information about a topic does not make one intelligent, as well as how the ‘reasoning’ associated with current LLMs does not. Googling something does not mean I know what I am talking about. Misnomers, marketing, and anthropomorphizing is what have led to this hype of &amp;lsquo;intelligence&amp;rsquo; in the first place. I am in no means downplaying the technological ability of LLMs, just defining it into a more correct category. LLMs have vastly improved productivity in many areas, albeit sacrificed for actual learning and understanding. They are a great tool for retrieval, search, summary, and any means of expression in general, but could in no means have the capacity to recognize, solve, and understand problems and replace humans with actual intelligence. We are in the midst of a revolution of expression, where people can spread, articulate, and prototype ideas, images, stories, and more faster than ever before. Let’s just make sure we don’t let ourselves be captivated by creative language masking as intelligence like some already are in other spheres of life.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;h6 id=&#34;ref1&#34;&gt;&lt;/h6&gt;
&lt;p&gt;[1] Marcus, Gary, and Ernest Davis. GPT-3, Bloviator: OpenAI’s language generator has no idea what it’s talking about. MIT Technology Review, 2020.
&lt;a href=&#34;https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/&#34;&gt;https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/&lt;/a&gt;&lt;/p&gt;
&lt;h6 id=&#34;ref2&#34;&gt;&lt;/h6&gt;
&lt;p&gt;[2] Searle, John. &amp;ldquo;Minds, brains, and programs.&amp;rdquo; Behavioral and Brain Sciences 3.3 (1980): 417–457.&lt;/p&gt;
&lt;h6 id=&#34;ref3&#34;&gt;&lt;/h6&gt;
&lt;p&gt;[3] Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., &amp;amp; Iwasawa, Y. (2022). &lt;em&gt;Large Language Models are Zero-Shot Reasoners&lt;/em&gt;. arXiv preprint arXiv:2205.11916. &lt;a href=&#34;https://arxiv.org/pdf/2205.11916&#34;&gt;PDF&lt;/a&gt;&lt;/p&gt;
&lt;h6 id=&#34;ref4&#34;&gt;&lt;/h6&gt;
&lt;p&gt;[4] Muennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Candès, E., &amp;amp; Hashimoto, T. (2025). &lt;em&gt;s1: Simple test-time scaling&lt;/em&gt;. arXiv preprint arXiv:2501.19393. &lt;a href=&#34;https://arxiv.org/pdf/2501.19393&#34;&gt;PDF&lt;/a&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Ghostty Nvim Config</title>
      <link>/posts/ghostty-nvim-config/</link>
      <pubDate>Sat, 18 Jan 2025 23:58:58 -0500</pubDate>
      
      <guid>/posts/ghostty-nvim-config/</guid>
      <description>&lt;p&gt;The Primeagen inspired me to start looking into better developer tools and actually start using nvim. Now I have adapted to ghostty and nvim and have come to really like it. Therefore, just want to document my config for when I switch devices or if anyone else actually happens to be interested. I could repeat myself here, but all you need is in my github repo &lt;a href=&#34;https://github.com/KarlVM12/ghostty-nvim-config&#34;&gt;ghostty-nvim-config&lt;/a&gt;.&lt;/p&gt;</description>
      <content>&lt;p&gt;The Primeagen inspired me to start looking into better developer tools and actually start using nvim. Now I have adapted to ghostty and nvim and have come to really like it. Therefore, just want to document my config for when I switch devices or if anyone else actually happens to be interested. I could repeat myself here, but all you need is in my github repo &lt;a href=&#34;https://github.com/KarlVM12/ghostty-nvim-config&#34;&gt;ghostty-nvim-config&lt;/a&gt;.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Dynamic Probabilistic Mapping Model</title>
      <link>/posts/dpmm/</link>
      <pubDate>Wed, 08 Jan 2025 22:50:20 -0500</pubDate>
      
      <guid>/posts/dpmm/</guid>
      <description>&lt;p&gt;Accurate classification across diverse domains is essential for information decision-making and effective resource allocation. With this, comes the Dynamic Probabilistic Mapping Model (DPMM), a flexible hierachial framework designed for multidomain predictions by analyzing feature-outcome correlations. DPMM employs a two-tiered architecture: a primary model initially categorizes each conclusion separately using one-hot encoded features mapped through probabilistic distributions. To address misclassifications and overlapping characteristics, the framework dynamically merges related classes based on performance metrics derived from confusion matrix analysis, and subsequently deploys specialized subclass models for refined predictions.&lt;/p&gt;</description>
      <content>&lt;p&gt;Accurate classification across diverse domains is essential for information decision-making and effective resource allocation. With this, comes the Dynamic Probabilistic Mapping Model (DPMM), a flexible hierachial framework designed for multidomain predictions by analyzing feature-outcome correlations. DPMM employs a two-tiered architecture: a primary model initially categorizes each conclusion separately using one-hot encoded features mapped through probabilistic distributions. To address misclassifications and overlapping characteristics, the framework dynamically merges related classes based on performance metrics derived from confusion matrix analysis, and subsequently deploys specialized subclass models for refined predictions.&lt;/p&gt;
&lt;p&gt;This hierarchial approach enables DPMM to adapt to varying data distributions and feature interactions, enhancing classification accuracy and reliability across multiple application areas such as healthcare, finance, and cybersecurity, allowing for the creation of domain specific AI agents. For instance, in an example healthcare scenario, the primary model achieved an initial accuracy of approximately 76% which can improve to around 87% after merging related classes. Subclass models further refine predictions, significantly boosting accuracy for specific condition groups. These results exemplify DPMM&amp;rsquo;s capability to continuously optimize its structure based on input data and outcome distributions. By integrating probabilistic feature mappings with dynamic class restructing, DPMM offers a robust and scalable solution for complex multi-class prediction tasks, ensuring higher precision and adaptability in diverse real-world applications.&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
