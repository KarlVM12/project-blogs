<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Language on Karl Muller</title>
    <link>/tags/language/</link>
    <description>Recent content in Language on Karl Muller</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 20 Jan 2026 22:00:00 -0500</lastBuildDate><atom:link href="/tags/language/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Greater Expression does not Lead to Greater Quality</title>
      <link>/posts/greater-expression-not-greater-quality/</link>
      <pubDate>Tue, 20 Jan 2026 22:00:00 -0500</pubDate>
      
      <guid>/posts/greater-expression-not-greater-quality/</guid>
      <description>&lt;p&gt;Last year, I wrote a small piece on how &lt;a href=&#34;/posts/llms-are-expression-not-intelligence/&#34;&gt;LLMs = Expression, not Intelligence&lt;/a&gt;. I feel like now is a more important time than ever to revisit this thought. Not that it was wrong, precisely the opposite, but because that ability of expression and creation through these models has only continued to increase, have put even more amazing tools in the hands of so many people, but we have yet to see the so called proof of better engineered systems. In fact, most likely the opposite has occurred.&lt;/p&gt;</description>
      <content>&lt;p&gt;Last year, I wrote a small piece on how &lt;a href=&#34;/posts/llms-are-expression-not-intelligence/&#34;&gt;LLMs = Expression, not Intelligence&lt;/a&gt;. I feel like now is a more important time than ever to revisit this thought. Not that it was wrong, precisely the opposite, but because that ability of expression and creation through these models has only continued to increase, have put even more amazing tools in the hands of so many people, but we have yet to see the so called proof of better engineered systems. In fact, most likely the opposite has occurred.&lt;/p&gt;
&lt;p&gt;LLMs have yet to make an impact purely because the force multiplier aspect is only a force multiplier for those that use it correctly. For many, it still comes off as &amp;ldquo;slop&amp;rdquo; or slowing other people down. This has become especially evident in an age where so many people have stopped asking questions, or at least the right ones. Those who leverage LLMs correctly are those who are naturally curious, who are asking questions and actually wanting to learn. If LLMs to one are simply just a &amp;ldquo;do this task for me&amp;rdquo;, then you are never going to learn anything, and the LLM is probably going to give you an undesirable output, leading to your frustration.&lt;/p&gt;
&lt;p&gt;This is why with how explosive tools like Claude Code have become, we don&amp;rsquo;t see an explosion of better content. And this has been proven throughout history as well. Every time we have reached a milestone in greater expression, it has often times lead to the fear of worse quality, known as the &amp;ldquo;media panic cycle&amp;rdquo;, commonly seen recently with the spread of misinformation and even going back to &lt;a href=&#34;https://newlearningonline.com/literacies/chapter-1/socrates-on-the-forgetfulness-that-comes-with-writing&#34;&gt;Socrates&amp;rsquo; fear of forgetfulness from written language&lt;/a&gt; around 350 BCE.&lt;/p&gt;
&lt;p&gt;Today, we would be in the &lt;strong&gt;&lt;em&gt;AI panic cycle&lt;/em&gt;&lt;/strong&gt;, where each new model sparks some fear of jobs being replaced coupled with one&amp;rsquo;s degradation of their abilities (for those that use them improperly), even though these models should be increasing one&amp;rsquo;s expression and creativity.&lt;/p&gt;
&lt;p&gt;It often reminds me of the quote from Dijkstra&amp;rsquo;s &lt;a href=&#34;https://www.cs.utexas.edu/~EWD/transcriptions/EWD06xx/EWD667.html&#34;&gt;&amp;ldquo;On the foolishness of &amp;rsquo;natural language programming&amp;rsquo;&amp;rdquo;&lt;/a&gt; from 1978:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;although changing to communication between machine and man conducted in the latter&amp;rsquo;s native tongue would greatly increase the machine&amp;rsquo;s burden, we have to challenge the assumption that this would simplify man&amp;rsquo;s life&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Even though Dijkstra wrote this in 1978, it precisely defines the current state of natural language programming. Language Models have to compute an enormous amount of work and energy in order to utilize them via natural language. It gives us the ability to greatly express ourselves, but we have stopped learning and asking questions.&lt;/p&gt;
&lt;p&gt;All these tools have only proven more so that coding was never truly the skill, that the skill is in the engineer themselves, and the lengths they will go to improve themselves. The ones unaffected by the current AI panic cycle use these tools wisely, to be the advanced assistant for greater self expression they are supposed to be, not to replace their own effort, motivation, or thought.&lt;/p&gt;
&lt;p&gt;When the industry starts to preach this more than the &amp;ldquo;replace all developers&amp;rdquo; ideology, that is when we will truly start to see the benefits.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Dijkstra, E. W. (1978). &lt;a href=&#34;https://www.cs.utexas.edu/~EWD/transcriptions/EWD06xx/EWD667.html&#34;&gt;On the foolishness of &amp;ldquo;natural language programming&amp;rdquo;&lt;/a&gt;. EWD667.&lt;/li&gt;
&lt;li&gt;Socrates on writing and memory (~350 BCE): &lt;a href=&#34;https://newlearningonline.com/literacies/chapter-1/socrates-on-the-forgetfulness-that-comes-with-writing&#34;&gt;The forgetfulness that comes with writing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Language &amp; LLMs = Expression, not Intelligence</title>
      <link>/posts/llms-are-expression-not-intelligence/</link>
      <pubDate>Sun, 04 May 2025 17:54:46 -0400</pubDate>
      
      <guid>/posts/llms-are-expression-not-intelligence/</guid>
      <description>&lt;p&gt;LLMs have been lauded for having intelligence and reasoning capabilities simply from choosing the next probable word in a sequence &lt;a href=&#34;#ref1&#34;&gt;[1]&lt;/a&gt;.
Formally, the APA defines intelligence as &amp;ldquo;the ability to derive information, learn from experience, adapt to the environment, understand, and correctly utilize thought and reason.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Language is certainly a powerful tool for communicating, spreading ideas, and technological advancement. Without language, ideas could never have moved as fast as they currently do. With this, some would then deduce that language then inherently leads towards intelligence, but this is simply not true. I do not study quantum mechanics, but I can read, memorize, and recite a proof if given enough to read, enough time to memorize, and a medium in order to regurgitate the information (speaking/writing English in this case). But that in no means actually proves I am intelligent or understand what I am saying or writing. With enough different sources of information and time to put it all together, I could probably also string together different memorized pieces of information to reach another piece of information, without ever having to actually understand any of it. In Searle&amp;rsquo;s Chinese Room Argument, a person inside a room can manipulate Chinese symbols following rules without ever actually understanding Chinese &lt;a href=&#34;#ref2&#34;&gt;[2]&lt;/a&gt;. This shows how language &amp;amp; syntax is not equal to semantics and understanding.&lt;/p&gt;</description>
      <content>&lt;p&gt;LLMs have been lauded for having intelligence and reasoning capabilities simply from choosing the next probable word in a sequence &lt;a href=&#34;#ref1&#34;&gt;[1]&lt;/a&gt;.
Formally, the APA defines intelligence as &amp;ldquo;the ability to derive information, learn from experience, adapt to the environment, understand, and correctly utilize thought and reason.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Language is certainly a powerful tool for communicating, spreading ideas, and technological advancement. Without language, ideas could never have moved as fast as they currently do. With this, some would then deduce that language then inherently leads towards intelligence, but this is simply not true. I do not study quantum mechanics, but I can read, memorize, and recite a proof if given enough to read, enough time to memorize, and a medium in order to regurgitate the information (speaking/writing English in this case). But that in no means actually proves I am intelligent or understand what I am saying or writing. With enough different sources of information and time to put it all together, I could probably also string together different memorized pieces of information to reach another piece of information, without ever having to actually understand any of it. In Searle&amp;rsquo;s Chinese Room Argument, a person inside a room can manipulate Chinese symbols following rules without ever actually understanding Chinese &lt;a href=&#34;#ref2&#34;&gt;[2]&lt;/a&gt;. This shows how language &amp;amp; syntax is not equal to semantics and understanding.&lt;/p&gt;
&lt;p&gt;When we apply this to LLMs, we can see this is also true. With enough time to train and tweak their parameters, weights, and biases, they can generate that information in their own medium. Once again, however, this does not equate to intelligence with the same following as the previous conclusion. Language in all cases serves only as a medium to transmit information and ideas, as a means of expression. LLMs can express and perform what some call ‘reasoning’ (explored below) across as many ideas as they want, but that in no way proceeds to assume intelligence, and especially far from Artificial Intelligence.&lt;/p&gt;
&lt;p&gt;Since we can see language as a means of expression simply does not, and can not by itself, lead to understanding, there is also the topic of tackling what is currently called ‘reasoning’ in the AI hype zeitgeist. Reasoning as it currently works in LLMs string together the next most probable strings of text based on its training, as well as throwing a couple of tricks in. Adding these tricks does not mean it is actually reasoning by any means. This perceived &amp;lsquo;reasoning&amp;rsquo; only exists simply because people were extending the also misnomer of &lt;em&gt;Chain of Thought&lt;/em&gt;. There might be a chain of words being strung together, but no thought process is actually occurring. For example, a simple trick of inserting ‘Let&amp;rsquo;s think step by step’ in the sequence of probabilities has resulted in an increased retrieval of a correct answer in LLMs &lt;a href=&#34;#ref3&#34;&gt;[3]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This is of course why prompt engineering works so well. If you insert a token of doubt into an incorrect answer, it will lean the probability towards a more correct answer. Since each token influences the conditional probability of the next, adjusting earlier tokens can shift the model’s output distribution toward more probable completions, and less towards &amp;lsquo;hallucinations&amp;rsquo;, as the anthropomorphized term goes. Therefore, when you throw a ‘wait’ &lt;a href=&#34;#ref4&#34;&gt;[4]&lt;/a&gt; in the following sequence, it leans even further in towards what would be the most correct probability according to its training. Tricks like these are by no means reasoning. Stringing together more probabilities will of course lead to improved retrieval because you start to touch on more of the parameters and tokens associated with the prompt. With more context and tokens surrounding a prompt, after all the so called ‘reasoning’ process, that will inherently lead to a better answer. So there is no actual reasoning going on, just more compute and tokens leveraged with a trick being spit out to reach that final answer.&lt;/p&gt;
&lt;p&gt;To round this out, language does not equate to understanding, and is far from intelligence. Regurgitating and retrieving more information about a topic does not make one intelligent, as well as how the ‘reasoning’ associated with current LLMs does not. Googling something does not mean I know what I am talking about. Misnomers, marketing, and anthropomorphizing is what have led to this hype of &amp;lsquo;intelligence&amp;rsquo; in the first place. I am in no means downplaying the technological ability of LLMs, just defining it into a more correct category. LLMs have vastly improved productivity in many areas, albeit sacrificed for actual learning and understanding. They are a great tool for retrieval, search, summary, and any means of expression in general, but could in no means have the capacity to recognize, solve, and understand problems and replace humans with actual intelligence. We are in the midst of a revolution of expression, where people can spread, articulate, and prototype ideas, images, stories, and more faster than ever before. Let’s just make sure we don’t let ourselves be captivated by creative language masking as intelligence like some already are in other spheres of life.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;h6 id=&#34;ref1&#34;&gt;&lt;/h6&gt;
&lt;p&gt;[1] Marcus, Gary, and Ernest Davis. GPT-3, Bloviator: OpenAI’s language generator has no idea what it’s talking about. MIT Technology Review, 2020.
&lt;a href=&#34;https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/&#34;&gt;https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/&lt;/a&gt;&lt;/p&gt;
&lt;h6 id=&#34;ref2&#34;&gt;&lt;/h6&gt;
&lt;p&gt;[2] Searle, John. &amp;ldquo;Minds, brains, and programs.&amp;rdquo; Behavioral and Brain Sciences 3.3 (1980): 417–457.&lt;/p&gt;
&lt;h6 id=&#34;ref3&#34;&gt;&lt;/h6&gt;
&lt;p&gt;[3] Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., &amp;amp; Iwasawa, Y. (2022). &lt;em&gt;Large Language Models are Zero-Shot Reasoners&lt;/em&gt;. arXiv preprint arXiv:2205.11916. &lt;a href=&#34;https://arxiv.org/pdf/2205.11916&#34;&gt;PDF&lt;/a&gt;&lt;/p&gt;
&lt;h6 id=&#34;ref4&#34;&gt;&lt;/h6&gt;
&lt;p&gt;[4] Muennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Candès, E., &amp;amp; Hashimoto, T. (2025). &lt;em&gt;s1: Simple test-time scaling&lt;/em&gt;. arXiv preprint arXiv:2501.19393. &lt;a href=&#34;https://arxiv.org/pdf/2501.19393&#34;&gt;PDF&lt;/a&gt;&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
